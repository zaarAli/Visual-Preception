{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "<br><br>\n",
    "Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from scipy.spatial.distance import cdist\n",
    "from numpy.linalg import svd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU\n",
    "\n",
    "<br>\n",
    "\n",
    "## Write a little code to see the difference between CPU and GPU\n",
    "\n",
    "My CPU: Intel® Core™ i7-10875H CPU @ 2.30GHz <br>\n",
    "My GPU: NVIDIA GeForce RTX 2080 Super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine device to run on ( GPU vs CPU )\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"Running tensors in \", device)\n",
    "\n",
    "matrix_size = 32*32\n",
    "x = torch.rand(matrix_size, matrix_size)\n",
    "y = torch.rand(matrix_size, matrix_size)\n",
    "\n",
    "print(\"******************* CPU *******************\")\n",
    "start = time.time()\n",
    "result = torch.matmul(x,y)\n",
    "print(time.time() - start)\n",
    "print(\"Verify device: \", result.device)\n",
    "\n",
    "x_gpu = x.to(device)\n",
    "y_gpu = y.to(device)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"******************* GPU *******************\")\n",
    "    start = time.time()\n",
    "    result_gpu = torch.matmul(x_gpu,y_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    print(time.time() - start)\n",
    "    print(\"Verify device: \", result_gpu.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array to torch tensor\n",
    "def to_torch_tensor(x, device=\"cuda\", dtype=torch.float32, requires_grad=False):\n",
    "    # return torch . from_numpy ( x ) . to ( device )\n",
    "    return torch.tensor(x, dtype=dtype, requires_grad=requires_grad).to(device)\n",
    "\n",
    "# torch tensor to numpy\n",
    "\n",
    "\n",
    "def to_numpy_array(x):\n",
    "    return x.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - Extracting Features and Matching using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_features_gpu(image):\n",
    "#     # Create a SIFT object and set the device to use the GPU\n",
    "#     sift = cv2.cuda_DescriptorMatcher.createBFMatcher(cv2.NORM_L2)\n",
    "#     sift.setPurgeScaleFactor(10)\n",
    "#     sift.setUseScaleOrientation(True)\n",
    "\n",
    "#     # Convert the input image to a GPU Mat object\n",
    "#     image_gpu = cv2.cuda_GpuMat()\n",
    "#     image_gpu.upload(image)\n",
    "\n",
    "#     # Detect keypoints and compute descriptors using SIFT on the GPU\n",
    "#     keypoints_gpu, descriptors_gpu = sift.detectWithDescriptors(image_gpu, None)\n",
    "\n",
    "#     # Download the keypoints and descriptors from the GPU to the CPU\n",
    "#     keypoints = keypoints_gpu.download()\n",
    "#     descriptors = descriptors_gpu.download()\n",
    "\n",
    "#     return keypoints, descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting keypoints and features using SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Extracts SIFT features from an image.\n",
    "\n",
    "    Args:\n",
    "        image: input image\n",
    "\n",
    "    Returns:\n",
    "        keypoints: list of SIFT keypoints\n",
    "        descriptors: SIFT descriptors for the keypoints\n",
    "    \"\"\"\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "    return keypoints, descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the best 1000 keypoints manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_N_features_manually(keypoints, descriptors, n):\n",
    "    \"\"\"\n",
    "    Extracts the top N SIFT features from an image based on their response.\n",
    "\n",
    "    Args:\n",
    "        keypoints: list of SIFT keypoints\n",
    "        descriptors: SIFT descriptors for the keypoints\n",
    "        n: number of top features to select\n",
    "\n",
    "    Returns:\n",
    "        keypoints_np: Numpy array of keypoints\n",
    "        N_keypoints: list of top N SIFT keypoints\n",
    "        N_features: SIFT descriptors for the top N keypoints\n",
    "    \"\"\"\n",
    "    # Sort keypoints by their response and select the top N keypoints\n",
    "    sorted_keypoints = sorted(\n",
    "        zip(keypoints, descriptors), key=lambda x: x[0].response, reverse=True)\n",
    "    N_keypoints, N_features = zip(*sorted_keypoints[:n])\n",
    "\n",
    "    # Convert the keypoints to a Numpy array\n",
    "    keypoints_np = cv2.KeyPoint_convert(keypoints)\n",
    "\n",
    "    return keypoints_np, N_keypoints, N_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the best 1000 keypoints by changing the parameters of the SIFT.CREATE class\n",
    "<br>\n",
    "\n",
    "After reading the documentation of OpenCV's [**SIFT's**](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html) [**cv::SIFT Class**](docs.opencv.org/3.4/d7/d60/classcv_1_1SIFT.html), I found out all the parameters which we can change for SIFT. <br><br>\n",
    "\n",
    "Moreover, I also found that we can do the same thing using **BRISK's** [**cv::BRISK Class**](https://docs.opencv.org/3.4/de/dbf/classcv_1_1BRISK.html)<br>\n",
    "\n",
    "BRISK is a feature point detection and description algorithm with scale invariance and rotation invariance, developed in 2011 as a ***free alternative*** to **SIFT** and readily implemented in famous CV libraries such as OpenCV.\n",
    "\n",
    "Both the implementations are as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_SIFT_parameters(image):\n",
    "    \"\"\"\n",
    "    Extracts SIFT features from an image with custom parameters.\n",
    "\n",
    "    Args:\n",
    "        image: input image\n",
    "\n",
    "    Returns:\n",
    "        keypoints: list of SIFT keypoints\n",
    "        descriptors: SIFT descriptors for the keypoints\n",
    "    \"\"\"\n",
    "    sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.02, nfeatures=1000)\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "    return keypoints, descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_BRISK_parameters(image):\n",
    "    \"\"\"\n",
    "    Extracts BRISK features from an image with custom parameters.\n",
    "\n",
    "    Args:\n",
    "        image: input image\n",
    "\n",
    "    Returns:\n",
    "        keypoints: list of BRISK keypoints\n",
    "        descriptors: BRISK descriptors for the keypoints\n",
    "    \"\"\"\n",
    "    brisk = cv2.BRISK_create(thresh=30, octaves=3, patternScale=1.0)\n",
    "    keypoints = brisk.detect(image, None)\n",
    "    keypoints = sorted(keypoints, key=lambda x: -x.response)[:1000]\n",
    "    keypoints, descriptors = brisk.compute(image, keypoints)\n",
    "\n",
    "    return keypoints, descriptors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_extractors(image):\n",
    "    \"\"\"\n",
    "    Runs multiple feature extraction algorithms on an input image.\n",
    "\n",
    "    Args:\n",
    "        image: input image\n",
    "\n",
    "    Returns:\n",
    "        keypoints: list of all keypoints extracted\n",
    "        descriptors: descriptors for all keypoints extracted\n",
    "        keypoints_np: numpy array of the best N keypoints\n",
    "        N_keypoints: list of the best N keypoints\n",
    "        N_features: descriptors for the best N keypoints\n",
    "        keypoints_SIFT: list of keypoints extracted using SIFT with custom parameters\n",
    "        descriptors_SIFT: descriptors for keypoints extracted using SIFT with custom parameters\n",
    "        keypoints_BRISK: list of keypoints extracted using BRISK with custom parameters\n",
    "        descriptors_BRISK: descriptors for keypoints extracted using BRISK with custom parameters\n",
    "    \"\"\"\n",
    "    # Pass the image to the SIFT feature extractor\n",
    "    keypoints, descriptors = extract_features(image)\n",
    "    print(\"Number of keypoints extracted in total:\", len(keypoints))\n",
    "\n",
    "    # Pass the keypoints to extract the 'N' best keypoints\n",
    "    keypoints_np, N_keypoints, N_features = extract_best_N_features_manually(\n",
    "        keypoints, descriptors, n=1000)\n",
    "    print(\"Number of keypoints after manually extracting the best 1000 keypoints:\", len(\n",
    "        N_keypoints))\n",
    "\n",
    "    # Pass the image to the SIFT feature extractor with the changed parameters\n",
    "    keypoints_SIFT, descriptors_SIFT = extract_features_SIFT_parameters(image)\n",
    "    print(\"Number of keypoints after changing the parameters of the SIFT function:\", len(\n",
    "        keypoints_SIFT))\n",
    "\n",
    "    # Pass the image to the BRISK feature extractor with the changed parameters\n",
    "    keypoints_BRISK, descriptors_BRISK = extract_features_BRISK_parameters(\n",
    "        image)\n",
    "    print(\"Number of keypoints using the BRISK class:\", len(keypoints_BRISK))\n",
    "\n",
    "    return keypoints, descriptors, keypoints_np, N_keypoints, N_features, keypoints_SIFT, descriptors_SIFT, keypoints_BRISK, descriptors_BRISK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Finding using Brute Force (one to all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(img1, img2, kp1, kp2, des1, des2, threshold):\n",
    "    \"\"\"\n",
    "    Finds matching features between two images based on their keypoints and descriptors.\n",
    "\n",
    "    Args:\n",
    "        img1: first image\n",
    "        kp1: keypoints of the first image\n",
    "        des1: descriptors of the first image\n",
    "        img2: second image\n",
    "        kp2: keypoints of the second image\n",
    "        des2: descriptors of the second image\n",
    "\n",
    "    Returns:\n",
    "        matches: list of matching features\n",
    "        matched_image: image showing the matches between the two input images\n",
    "    \"\"\"\n",
    "    # Check that the keypoints and descriptors are not empty\n",
    "    if not (len(kp1) and len(kp2) and len(des1) and len(des2)):\n",
    "        print(\"Empty keypoints or descriptors\")\n",
    "        return [], None\n",
    "\n",
    "    # Compute the distances between each pair of descriptors\n",
    "    distances = cdist(des1, des2, 'euclidean')\n",
    "\n",
    "    # Set the threshold dynamically based on the distribution of distances\n",
    "    threshold = np.median(distances) * threshold\n",
    "\n",
    "    matches = []\n",
    "    for i in range(len(des1)):\n",
    "        # Find the index of the closest descriptor in the second image\n",
    "        idx = np.argmin(distances[i])\n",
    "\n",
    "        # If the distance is below the threshold, add the match\n",
    "        distance = distances[i][idx]\n",
    "        if distance < threshold:\n",
    "            matches.append(cv2.DMatch(i, idx, distance))\n",
    "\n",
    "    # Draw the matches as lines between corresponding key points\n",
    "    if matches:\n",
    "        matched_image = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    else:\n",
    "        matched_image = None\n",
    "\n",
    "    return matches, matched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_1(img1, kp1, des1, img2, kp2, des2, threshold):\n",
    "    \"\"\"\n",
    "    Finds matches between keypoints in two images using Euclidean distance.\n",
    "\n",
    "    Args:\n",
    "        img1: first image\n",
    "        kp1: keypoints in first image\n",
    "        des1: descriptors for keypoints in first image\n",
    "        img2: second image\n",
    "        kp2: keypoints in second image\n",
    "        des2: descriptors for keypoints in second image\n",
    "\n",
    "    Returns:\n",
    "        matches: list of matched keypoints (cv2.DMatch objects)\n",
    "        matched_image: image with matched keypoints drawn\n",
    "    \"\"\"\n",
    "    # Check that the keypoints and descriptors are not empty\n",
    "    if not (len(kp1) and len(kp2) and len(des1) and len(des2)):\n",
    "        print(\"Empty keypoints or descriptors\")\n",
    "        return [], None\n",
    "\n",
    "    # Compute the distances between each pair of descriptors\n",
    "    distances = cdist(des1, des2, 'euclidean')\n",
    "\n",
    "    # Set the threshold dynamically based on the distribution of distances\n",
    "    threshold = np.median(distances) * threshold\n",
    "\n",
    "    matches = []\n",
    "    for i in range(len(kp1)):\n",
    "        # Compute the distances between the descriptor i from image 1 and all descriptors from image 2\n",
    "        distances_i = np.linalg.norm(des2 - des1[i], axis=1)\n",
    "\n",
    "        if np.min(distances_i) < threshold:\n",
    "            j = np.argmin(distances_i)\n",
    "            matches.append(cv2.DMatch(i, j, np.min(distances_i)))\n",
    "\n",
    "    matched_image = cv2.drawMatches(\n",
    "        img1, kp1, img2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    return matches, matched_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both functions *find_matches()* and *find_matches_1()* perform feature matching between two images based on their keypoints and descriptors using the **Brute force** approach which is also called **one to all**, but they differ in the way they calculate the distances between the descriptors.\n",
    "\n",
    "\n",
    "### Differences:\n",
    "\n",
    "- *find_matches* takes two images and their corresponding key points and descriptors, while *find_matches_1* takes each image's key points and descriptors separately.\n",
    "- In *find_matches*, the distances between each pair of descriptors are computed once and stored in a matrix, while in *find_matches_1*, the distances between each descriptor in the first image and all descriptors in the second image are computed for each descriptor in the first image.\n",
    "- *find_matches* uses **np.argmin** to find the index of the closest descriptor in the second image for each descriptor in the first image, while *find_matches_1* uses **np.linalg.norm** to compute the distances between each descriptor in the first image and all descriptors in the second image, and then uses **np.argmin** to find the index of the closest descriptor in the second image for each descriptor in the first image.\n",
    "\n",
    "### Advantages of find_matches (the first function):\n",
    "\n",
    "- It only computes the distances between descriptors once, which can be faster if the number of descriptors is large.\n",
    "- It takes two images and their corresponding key points and descriptors, which can be more convenient if the images are already loaded and the key points and descriptors are computed elsewhere.\n",
    "\n",
    "### Disadvantages of find_matches:\n",
    "\n",
    "- It may not be as accurate as find_matches_1 since it only looks for the closest descriptor in the second image for each descriptor in the first image, while there may be a better match that is further away.\n",
    "- It requires more memory since it stores the distances between descriptors in a matrix.\n",
    "\n",
    "### Advantages of find_matches_1 (The second function):\n",
    "\n",
    "- It can be more accurate since it considers all descriptors in the second image for each descriptor in the first image.\n",
    "- It requires less memory since it only computes the distances between each descriptor in the first image and all descriptors in the second image, rather than storing all distances in a matrix.\n",
    "\n",
    "### Disadvantages of find_matches_1:\n",
    "\n",
    "- It can be slower if the number of descriptors is large since it computes the distances between each descriptor in the first image and all descriptors in the second image for each descriptor in the first image.\n",
    "- It takes the key points and descriptors for each image separately, which may require extra steps if they are not computed yet.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_find_matchers(Image_1, Image_2, keypoints_SIFT_1, keypoints_SIFT_2, descriptors_SIFT_1, descriptors_SIFT_2, threshold_1, threshold_2, x=30):\n",
    "    \"\"\"\n",
    "    Runs three different matching functions and plots the results.\n",
    "\n",
    "    Args:\n",
    "        Image_1: first image\n",
    "        Image_2: second image\n",
    "        keypoints_SIFT_1: keypoints of the first image\n",
    "        keypoints_SIFT_2: keypoints of the second image\n",
    "        descriptors_SIFT_1: descriptors of the first image\n",
    "        descriptors_SIFT_2: descriptors of the second image\n",
    "        x: optional parameter for font size in the plot titles (default: 30)\n",
    "\n",
    "    Returns:\n",
    "        good_matches_1: list of good matches found using the first function\n",
    "        matched_image_1: image showing the matches found using the first function\n",
    "        good_matches_2: list of good matches found using the second function\n",
    "        matched_image_2: image showing the matches found using the second function\n",
    "        good_matches_3: list of good matches found using the third function (using built-in)\n",
    "        matched_image_3: image showing the matches found using the third function (using built-in)\n",
    "    \"\"\"\n",
    "    good_matches_1, matched_image_1 = find_matches(\n",
    "        Image_1, Image_2, keypoints_SIFT_1, keypoints_SIFT_2, descriptors_SIFT_1, descriptors_SIFT_2, threshold_1)\n",
    "\n",
    "    good_matches_2, matched_image_2 = find_matches_1(\n",
    "        Image_1, keypoints_SIFT_1, descriptors_SIFT_1, Image_2, keypoints_SIFT_2, descriptors_SIFT_2, threshold_2)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"The number of good matches found using the first function are: \", len(\n",
    "        good_matches_1))\n",
    "    print(\"The number of good matches found using the second function are: \", len(\n",
    "        good_matches_2))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(\n",
    "        Image_2.shape[1]/25, Image_2.shape[0]/25))\n",
    "\n",
    "    # Plot the first image\n",
    "    axs[0].imshow(cv2.cvtColor(matched_image_1, cv2.COLOR_BGR2RGB))\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('First Function - Distance computed once', fontsize=x)\n",
    "\n",
    "    # Plot the second image\n",
    "    axs[1].imshow(cv2.cvtColor(matched_image_2, cv2.COLOR_BGR2RGB))\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\n",
    "        \"Second Function - Computed for every descriptor\", fontsize=x)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "    return good_matches_1, matched_image_1, good_matches_2, matched_image_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the match finding function to implement 1NN/2NN ratio test\n",
    "\n",
    "By doing this, we can remove some ambiguous correspondences inside the *match finding* functions. <br>\n",
    "<br>\n",
    "The 1NN/2NN ratio test is a technique used in computer vision and image processing to remove ambiguous matches between the features of two images.<br><br>\n",
    "\n",
    "When finding matches between the features of two images, a common technique is to compare the descriptors of each feature and find the closest match in the other image. However, sometimes the closest match is not the correct match and can lead to false matches. To address this, the 1NN/2NN ratio test compares the distances of the two closest matches for each feature, and removes the match if the distance ratio between the two matches is above a certain threshold.<br><br>\n",
    "\n",
    "The 1NN/2NN ratio test works as follows: for each feature in the first image, we find the two closest matches in the second image. We then compute the ratio of the distance between the first and second matches. If this ratio is below a threshold (typically 0.8 or 0.7), we accept the match as valid. Otherwise, we reject the match as ambiguous.<br><br>\n",
    "\n",
    "The 1NN/2NN ratio test removes ambiguous matches by ensuring that the best match is significantly better than the second best match. This is based on the assumption that the correct match should be much closer to the feature than any other incorrect match. By using a ratio test, we can avoid false matches that may result from features that have similar descriptors or from noise in the images.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_filtered(img1, img2, kp1, kp2, des1, des2, threshold, ratio=0.75):\n",
    "    \"\"\"\n",
    "    Finds matching features between two images based on their keypoints and descriptors.\n",
    "\n",
    "    Args:\n",
    "        img1: first image\n",
    "        kp1: keypoints of the first image\n",
    "        des1: descriptors of the first image\n",
    "        img2: second image\n",
    "        kp2: keypoints of the second image\n",
    "        des2: descriptors of the second image\n",
    "        threshold: threshold used to filter matches based on descriptor distance\n",
    "        ratio: threshold used to remove ambiguous matches based on 1NN/2NN distance ratio\n",
    "\n",
    "    Returns:\n",
    "        matches: list of matching features\n",
    "        matched_image: image showing the matches between the two input images\n",
    "    \"\"\"\n",
    "    # Check that the keypoints and descriptors are not empty\n",
    "    if not (len(kp1) and len(kp2) and len(des1) and len(des2)):\n",
    "        print(\"Empty keypoints or descriptors\")\n",
    "        return [], None\n",
    "\n",
    "    # Compute the distances between each pair of descriptors\n",
    "    distances = cdist(des1, des2, 'euclidean')\n",
    "\n",
    "    # Set the threshold dynamically based on the distribution of distances\n",
    "    threshold = np.median(distances) * threshold\n",
    "\n",
    "    # Perform 1NN/2NN ratio test to remove ambiguous matches\n",
    "    matches = []\n",
    "    for i in range(len(des1)):\n",
    "        # Find the indices of the closest and second closest descriptors in the second image\n",
    "        sorted_indices = np.argsort(distances[i])\n",
    "        closest_idx, second_closest_idx = sorted_indices[0], sorted_indices[1]\n",
    "\n",
    "        # Calculate the distance ratio between the closest and second closest descriptors\n",
    "        distance_ratio = distances[i][closest_idx] / distances[i][second_closest_idx]\n",
    "\n",
    "        # If the distance ratio is below the threshold and the closest distance is below the threshold, add the match\n",
    "        closest_distance = distances[i][closest_idx]\n",
    "        if distance_ratio < ratio and closest_distance < threshold:\n",
    "            matches.append(cv2.DMatch(i, closest_idx, closest_distance))\n",
    "\n",
    "    # Draw the matches as lines between corresponding key points\n",
    "    if matches:\n",
    "        matched_image = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    else:\n",
    "        matched_image = None\n",
    "\n",
    "    return matches, matched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_1_filtered(img1, kp1, des1, img2, kp2, des2, threshold, ratio=0.75):\n",
    "    \"\"\"\n",
    "    Finds matches between keypoints in two images using Euclidean distance and 1NN/2NN ratio test.\n",
    "\n",
    "    Args:\n",
    "        img1: first image\n",
    "        kp1: keypoints in first image\n",
    "        des1: descriptors for keypoints in first image\n",
    "        img2: second image\n",
    "        kp2: keypoints in second image\n",
    "        des2: descriptors for keypoints in second image\n",
    "        threshold: distance threshold for matching keypoints\n",
    "        ratio: 1NN/2NN ratio threshold for removing ambiguous matches (default 0.75)\n",
    "\n",
    "    Returns:\n",
    "        matches: list of matched keypoints (cv2.DMatch objects)\n",
    "        matched_image: image with matched keypoints drawn\n",
    "    \"\"\"\n",
    "    # Check that the keypoints and descriptors are not empty\n",
    "    if not (len(kp1) and len(kp2) and len(des1) and len(des2)):\n",
    "        print(\"Empty keypoints or descriptors\")\n",
    "        return [], None\n",
    "\n",
    "    # Compute the distances between each pair of descriptors\n",
    "    distances = cdist(des1, des2, 'euclidean')\n",
    "\n",
    "    # Set the threshold dynamically based on the distribution of distances\n",
    "    threshold = np.median(distances) * threshold\n",
    "\n",
    "    matches = []\n",
    "    for i in range(len(kp1)):\n",
    "        # Compute the distances between the descriptor i from image 1 and all descriptors from image 2\n",
    "        distances_i = np.linalg.norm(des2 - des1[i], axis=1)\n",
    "\n",
    "        # Find the indices of the two closest descriptors in the second image\n",
    "        indices = np.argsort(distances_i)[:2]\n",
    "\n",
    "        # Check if the distance ratio is below the threshold\n",
    "        if distances_i[indices[0]] < ratio * distances_i[indices[1]] and distances_i[indices[0]] < threshold:\n",
    "            matches.append(cv2.DMatch(i, indices[0], distances_i[indices[0]]))\n",
    "\n",
    "    matched_image = cv2.drawMatches(\n",
    "        img1, kp1, img2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    return matches, matched_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches_using_builtin_function(image1, image2, keypoints1, descriptors1, keypoints2, descriptors2, threshold):\n",
    "    \"\"\"\n",
    "    Finds good matches between the descriptors of two input images using the brute force matcher algorithm.\n",
    "\n",
    "    Args:\n",
    "        image1: first input image\n",
    "        image2: second input image\n",
    "        keypoints1: keypoints of the first image\n",
    "        descriptors1: descriptors of the first image\n",
    "        keypoints2: keypoints of the second image\n",
    "        descriptors2: descriptors of the second image\n",
    "        max_distance: maximum allowed distance between the descriptors\n",
    "\n",
    "    Returns:\n",
    "        good_matches: list of good matches between the descriptors of the two images\n",
    "        matched_image: image showing the good matches between the two input images\n",
    "    \"\"\"\n",
    "    # Create a brute force matcher object\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "    # Match the descriptors of the two images\n",
    "    matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "    # Filter the matches based on the distance between the descriptors\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < threshold * n.distance:\n",
    "            good_matches.append([m])\n",
    "\n",
    "    # Draw the matches on the images\n",
    "    matched_image = cv2.drawMatchesKnn(image1, keypoints1, image2, keypoints2,\n",
    "                                       good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    return good_matches, matched_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_find_matchers_filtered(Image_1, Image_2, keypoints_SIFT_1, keypoints_SIFT_2, descriptors_SIFT_1, descriptors_SIFT_2, threshold_1, threshold_2, threshold_3, x=30):\n",
    "    \"\"\"\n",
    "    Runs three different filtered matching functions and plots the results.\n",
    "\n",
    "    Args:\n",
    "        Image_1: first image\n",
    "        Image_2: second image\n",
    "        keypoints_SIFT_1: keypoints of the first image\n",
    "        keypoints_SIFT_2: keypoints of the second image\n",
    "        descriptors_SIFT_1: descriptors of the first image\n",
    "        descriptors_SIFT_2: descriptors of the second image\n",
    "        x: optional parameter for font size in the plot titles (default: 30)\n",
    "\n",
    "    Returns:\n",
    "        good_matches_1_filtered: list of good matches found using the first function\n",
    "        matched_image_1_filtered: image showing the matches found using the first function\n",
    "        good_matches_2_filtered: list of good matches found using the second function\n",
    "        matched_image_2_filtered: image showing the matches found using the second function\n",
    "        good_matches_3_built_in: list of good matches found using the third function (using built-in)\n",
    "        matched_image_3_built_in: image showing the matches found using the third function (using built-in)\n",
    "    \"\"\"\n",
    "    good_matches_1_filtered, matched_image_1_filtered = find_matches_filtered(\n",
    "        Image_1, Image_2, keypoints_SIFT_1, keypoints_SIFT_2, descriptors_SIFT_1, descriptors_SIFT_2, threshold_1)\n",
    "\n",
    "    good_matches_2_filtered, matched_image_2_filtered = find_matches_1_filtered(\n",
    "        Image_1, keypoints_SIFT_1, descriptors_SIFT_1, Image_2, keypoints_SIFT_2, descriptors_SIFT_2, threshold_2)\n",
    "\n",
    "    good_matches_3_built_in, matched_image_3_built_in = find_matches_using_builtin_function(\n",
    "        Image_1, Image_2, keypoints_SIFT_1, descriptors_SIFT_1, keypoints_SIFT_2, descriptors_SIFT_2, threshold_3)\n",
    "\n",
    "    print(\"The number of good matches found after using 1NN/2NN ratio test on the first function are: \", len(\n",
    "        good_matches_1_filtered))\n",
    "    print(\"The number of good matches found after using 1NN/2NN ratio test on the second function are: \", len(\n",
    "        good_matches_2_filtered))\n",
    "    print(\"The number of good matches found using the third function (using built-in) are: \",\n",
    "          len(good_matches_3_built_in))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(\n",
    "        Image_2.shape[1]/25, Image_2.shape[0]/25))\n",
    "\n",
    "    # Plot the first image\n",
    "    axs[0].imshow(cv2.cvtColor(matched_image_1_filtered, cv2.COLOR_BGR2RGB))\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('First Function - Distance computed once', fontsize=x)\n",
    "\n",
    "    # Plot the second image\n",
    "    axs[1].imshow(cv2.cvtColor(matched_image_2_filtered, cv2.COLOR_BGR2RGB))\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(\n",
    "        \"Second Function -  Computed for every descriptor\", fontsize=x)\n",
    "\n",
    "    # Plot the third image\n",
    "    axs[2].imshow(cv2.cvtColor(matched_image_3_built_in, cv2.COLOR_BGR2RGB))\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(\"Using Built-in Functions\", fontsize=x)\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "    return good_matches_1_filtered, matched_image_1_filtered, good_matches_2_filtered,\\\n",
    "          matched_image_2_filtered, good_matches_3_built_in, matched_image_3_built_in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the keypoints on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawKeypoints(image, keypoints):\n",
    "    \"\"\"\n",
    "    Draws keypoints on an image.\n",
    "\n",
    "    Args:\n",
    "        image: image to draw keypoints on\n",
    "        keypoints: keypoints to draw\n",
    "\n",
    "    Returns:\n",
    "        result: image with keypoints drawn on it\n",
    "    \"\"\"\n",
    "    result = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints_subplot(image, result1, result2, result3, result4, x=20):\n",
    "    \"\"\"\n",
    "    Plots the results.\n",
    "\n",
    "    Args:\n",
    "        image: image to draw keypoints on\n",
    "        result1: result of draw_keypoints function\n",
    "        result2: result of draw_keypoints function\n",
    "        result3: result of draw_keypoints function\n",
    "        result4: result of draw_keypoints function\n",
    "        x: optional parameter for font size in the plot titles (default: 20) \n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(\n",
    "        image.shape[1]/30, image.shape[0]/30))\n",
    "    ax[0, 0].imshow(cv2.cvtColor(result1, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[0, 0].axis('off')\n",
    "    ax[0, 1].imshow(cv2.cvtColor(result2, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[0, 1].axis('off')\n",
    "    ax[1, 0].imshow(cv2.cvtColor(result3, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[1, 0].axis('off')\n",
    "    ax[1, 1].imshow(cv2.cvtColor(result4, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[1, 1].axis('off')\n",
    "    ax[0, 0].set_title('All Key points', fontsize=x)\n",
    "    ax[0, 1].set_title('Best 1000 keypoints', fontsize=x)\n",
    "    ax[1, 0].set_title('SIFT parameters', fontsize=x)\n",
    "    ax[1, 1].set_title('BRISK', fontsize=x)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Image **Van Gogh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image\n",
    "ref_image = cv2.imread('data/img_ref.jpg')\n",
    "\n",
    "keypoints, descriptors,\\\n",
    "    keypoints_np, N_keypoints, N_features,\\\n",
    "    keypoints_SIFT, descriptors_SIFT,\\\n",
    "    keypoints_BRISK, descriptors_BRISK = run_all_extractors(ref_image)\n",
    "\n",
    "# Generate the image data for each image\n",
    "\n",
    "result1 = drawKeypoints(ref_image, keypoints)\n",
    "result2 = drawKeypoints(ref_image, N_keypoints)\n",
    "result3 = drawKeypoints(ref_image, keypoints_SIFT)\n",
    "result4 = drawKeypoints(ref_image, keypoints_BRISK)\n",
    "\n",
    "plot_keypoints_subplot(ref_image, result1, result2, result3, result4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ESIREM logo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image and convert it to Gray-Scale\n",
    "E_logo = cv2.imread('data/esirem.jpg')\n",
    "\n",
    "keypoints_E, descriptors_E,\\\n",
    "    keypoints_np_E, N_keypoints_E, N_features_E,\\\n",
    "    keypoints_SIFT_E, descriptors_SIFT_E,\\\n",
    "    keypoints_BRISK_E, descriptors_BRISK_E = run_all_extractors(E_logo)\n",
    "\n",
    "# Generate the image data for each image\n",
    "\n",
    "result1_E = drawKeypoints(E_logo, keypoints_E)\n",
    "result2_E = drawKeypoints(E_logo, N_keypoints_E)\n",
    "result3_E = drawKeypoints(E_logo, keypoints_SIFT_E)\n",
    "result4_E = drawKeypoints(E_logo, keypoints_BRISK_E)\n",
    "\n",
    "plot_keypoints_subplot(E_logo, result1_E, result2_E, result3_E, result4_E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Img_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image and convert it to Gray-Scale\n",
    "img_1 = cv2.imread('data/img_1.jpg')\n",
    "\n",
    "keypoints_1, descriptors_1,\\\n",
    "    keypoints_np_1, N_keypoints_1, N_features_1,\\\n",
    "    keypoints_SIFT_1, descriptors_SIFT_1,\\\n",
    "    keypoints_BRISK_1, descriptors_BRISK_1 = run_all_extractors(img_1)\n",
    "\n",
    "# Generate the image data for each image\n",
    "\n",
    "result1_1 = drawKeypoints(img_1, keypoints_1)\n",
    "result2_1 = drawKeypoints(img_1, N_keypoints_1)\n",
    "result3_1 = drawKeypoints(img_1, keypoints_SIFT_1)\n",
    "result4_1 = drawKeypoints(img_1, keypoints_BRISK_1)\n",
    "\n",
    "plot_keypoints_subplot(img_1, result1_1, result2_1, result3_1, result4_1, x=75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image\n",
    "img_2 = cv2.imread('data/img_2.png')\n",
    "\n",
    "keypoints_2, descriptors_2,\\\n",
    "    keypoints_np_2, N_keypoints_2, N_features_3,\\\n",
    "    keypoints_SIFT_2, descriptors_SIFT_2,\\\n",
    "    keypoints_BRISK_2, descriptors_BRISK_2 = run_all_extractors(img_2)\n",
    "\n",
    "# Generate the image data for each image\n",
    "\n",
    "result1_2 = drawKeypoints(img_2, keypoints_2)\n",
    "result2_2 = drawKeypoints(img_2, N_keypoints_2)\n",
    "result3_2 = drawKeypoints(img_2, keypoints_SIFT_2)\n",
    "result4_2 = drawKeypoints(img_2, keypoints_BRISK_2)\n",
    "\n",
    "plot_keypoints_subplot(img_2, result1_2, result2_2, result3_2, result4_2, x=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image\n",
    "img_3 = cv2.imread('data/img_3.jpg')\n",
    "\n",
    "keypoints_3, descriptors_3,\\\n",
    "    keypoints_np_3, N_keypoints_3, N_features_3,\\\n",
    "    keypoints_SIFT_3, descriptors_SIFT_3,\\\n",
    "    keypoints_BRISK_3, descriptors_BRISK_3 = run_all_extractors(img_3)\n",
    "\n",
    "# Generate the image data for each image\n",
    "\n",
    "result1_3 = drawKeypoints(img_3, keypoints_3)\n",
    "result2_3 = drawKeypoints(img_3, N_keypoints_3)\n",
    "result3_3 = drawKeypoints(img_3, keypoints_SIFT_3)\n",
    "result4_3 = drawKeypoints(img_3, keypoints_BRISK_3)\n",
    "\n",
    "plot_keypoints_subplot(img_3, result1_3, result2_3, result3_3, result4_3, x=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Img_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image\n",
    "img_4 = cv2.imread('data/img_4.jpg')\n",
    "\n",
    "keypoints_4, descriptors_4,\\\n",
    "    keypoints_np_4, N_keypoints_4, N_features_4,\\\n",
    "    keypoints_SIFT_4, descriptors_SIFT_4,\\\n",
    "    keypoints_BRISK_4, descriptors_BRISK_4 = run_all_extractors(img_4)\n",
    "\n",
    "# Generate the image data for each image\n",
    "\n",
    "result1_4 = drawKeypoints(img_4, keypoints_4)\n",
    "result2_4 = drawKeypoints(img_4, N_keypoints_4)\n",
    "result3_4 = drawKeypoints(img_4, keypoints_SIFT_4)\n",
    "result4_4 = drawKeypoints(img_4, keypoints_BRISK_4)\n",
    "\n",
    "plot_keypoints_subplot(img_4, result1_4, result2_4, result3_4, result4_4, x=75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Question:** Are there differences regarding the number of detected keypoints using the default parameters?. <br><br>\n",
    "\n",
    "**Answer:** Yes, there will be differences in the number of detected keypoints in the two codes.<br>\n",
    "\n",
    "- extract_features_SIFT_parameters(image) is using SIFT with nfeatures=1000, which means that it will detect up to 1000 keypoints in the image, regardless of their quality.<br> \n",
    "\n",
    "- On the other hand, extract_features(img, n=1000) is using SIFT without any limit on the number of keypoints detected. It then sorts the detected keypoints by their response and only selects the top N keypoints, where N is a parameter passed to the function.<br>\n",
    "\n",
    " - This means that the number of keypoints selected in the extract_features will depend on the quality of the detected keypoints, and may be less than or greater than 1000 depending on the image and the value of n.<br>\n",
    "\n",
    " - On the other hand we have BRISK, whose results are not as good as SIFT but is an alternative available. Ofcourse, now that SIFT is free to use, we should use SIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfiltered -- Without 1NN/2NN Ratio test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches for image one and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_good_matches_1, img_1_matched_image_1, \\\n",
    "    img_1_good_matches_2, img_1_matched_image_2, = \\\n",
    "    run_all_find_matchers(ref_image, img_1, keypoints_SIFT,\n",
    "                          keypoints_SIFT_1, descriptors_SIFT, \n",
    "                          descriptors_SIFT_1, threshold_1 = 0.5, \n",
    "                          threshold_2 = 0.5, x = 75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches for image two and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2_good_matches_1, img_2_matched_image_1, \\\n",
    "    img_2_good_matches_2, img_2_matched_image_2, = \\\n",
    "    run_all_find_matchers(ref_image, img_2, keypoints_SIFT,\n",
    "                          keypoints_SIFT_2, descriptors_SIFT, \n",
    "                          descriptors_SIFT_2, threshold_1 = 0.5, \n",
    "                          threshold_2 = 0.5, x=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches for image three and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_3_good_matches_1, img_3_matched_image_1, \\\n",
    "    img_3_good_matches_2, img_3_matched_image_2,  = \\\n",
    "    run_all_find_matchers(ref_image, img_3, keypoints_SIFT,\n",
    "                          keypoints_SIFT_3, descriptors_SIFT, \n",
    "                          descriptors_SIFT_3, threshold_1 = 0.5,\n",
    "                          threshold_2 = 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matches for image four and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_4_good_matches_1, img_4_matched_image_1, \\\n",
    "    img_4_good_matches_2, img_4_matched_image_2,  = \\\n",
    "    run_all_find_matchers(ref_image, img_4, keypoints_SIFT,\n",
    "                          keypoints_SIFT_4, descriptors_SIFT, \n",
    "                          descriptors_SIFT_4, threshold_1 = 0.5, \n",
    "                          threshold_2 = 0.5, x=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered -- With 1NN/2NN Ratio test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Matches for image one and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_good_matches_1_filtered, img_1_matched_image_1_filtered, \\\n",
    "    img_1_good_matches_2_filtered, img_1_matched_image_2_filtered, \\\n",
    "    img_1_good_matches_3_built_in, img_1_matched_image_3_built_in = \\\n",
    "    run_all_find_matchers_filtered(ref_image, img_1, keypoints_SIFT,\n",
    "                          keypoints_SIFT_1, descriptors_SIFT, \n",
    "                          descriptors_SIFT_1, threshold_1 = 0.5, \n",
    "                          threshold_2 = 0.5, threshold_3 = 0.75, x = 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Matches for image two and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2_good_matches_1_filtered, img_2_matched_image_1_filtered, \\\n",
    "    img_2_good_matches_2_filtered, img_2_matched_image_2_filtered, \\\n",
    "    img_2_good_matches_3_built_in, img_2_matched_image_3_built_in = \\\n",
    "    run_all_find_matchers_filtered(ref_image, img_2, keypoints_SIFT,\n",
    "                          keypoints_SIFT_2, descriptors_SIFT, \n",
    "                          descriptors_SIFT_2, threshold_1 = 0.5, \n",
    "                          threshold_2 = 0.5, threshold_3 = 0.75, x=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Matches for image three and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_3_good_matches_1_filtered, img_3_matched_image_1_filtered, \\\n",
    "    img_3_good_matches_2_filtered, img_3_matched_image_2_filtered, \\\n",
    "    img_3_good_matches_3_built_in, img_3_matched_image_3_built_in = \\\n",
    "    run_all_find_matchers_filtered(ref_image, img_3, keypoints_SIFT,\n",
    "                          keypoints_SIFT_3, descriptors_SIFT, \n",
    "                          descriptors_SIFT_3, threshold_1 = 0.5,\n",
    "                          threshold_2 = 0.5, threshold_3 = 0.75, x = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtered Matches for image four and reference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_4_good_matches_1_filtered, img_4_matched_image_1_filtered, \\\n",
    "    img_4_good_matches_2_filtered, img_4_matched_image_2_filtered, \\\n",
    "    img_4_good_matches_3_built_in, img_4_matched_image_3_built_in = \\\n",
    "    run_all_find_matchers_filtered(ref_image, img_4, keypoints_SIFT,\n",
    "                          keypoints_SIFT_4, descriptors_SIFT, \n",
    "                          descriptors_SIFT_4, threshold_1 = 0.5, \n",
    "                          threshold_2 = 0.5, threshold_3 = 0.75, x=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difference between filter and unfiltered matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The difference between the Filtered and Unfiltered Matches Response for the 1st picture is: \\n\",\\\n",
    "      len(img_1_good_matches_1)  - len(img_1_good_matches_1_filtered), 'ambiguous matches')\n",
    "\n",
    "print(\"The difference between the Filtered and Unfiltered Matches Response for the second picture is: \\n\",\\\n",
    "      len(img_2_good_matches_1)  - len(img_2_good_matches_1_filtered), 'ambiguous matches')\n",
    "\n",
    "print(\"The difference between the Filtered and Unfiltered Matches Response for the 3rd picture is: \\n\",\\\n",
    "      len(img_3_good_matches_1)  - len(img_3_good_matches_1_filtered), 'ambiguous matches')\n",
    "\n",
    "print(\"The difference between the Filtered and Unfiltered Matches Response for the 4th picture is: \\n\",\\\n",
    "      len(img_4_good_matches_1)  - len(img_4_good_matches_1_filtered), 'ambiguous matches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can see that by using the 1NN/2NN ratio test, we are able to remove many ambiguous matches\n",
    "2. Also, we can see that after applying the $\\frac{1NN}{2NN}$ ratio test, the results are almost the same as the built in functions provided by OpenCV library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Robust 2D Homography Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "[**Prof. Dr. Cyrill Stachniss**](https://www.ipb.uni-bonn.de/people/cyrill-stachniss/)<br><br>\n",
    "\n",
    "[**Photogrammetry** by Prof. Dr. Cyrill Stachniss](https://www.ipb.uni-bonn.de/photo12-2021/)<br><br>\n",
    "\n",
    "[**Direct Linear Transform** -DLT- for Camera Calibration and Localization Slides by Prof. Dr. Cyrill Stachniss](https://www.ipb.uni-bonn.de/html/teaching/photo12-2021/2021-pho1-21-DLT.pptx.pdf)  <br><br>\n",
    "\n",
    "[**RANSAC** – Random Sample Consensus by Prof. Dr. Cyrill Stachniss](https://www.ipb.uni-bonn.de/html/teaching/photo12-2021/2021-pho2-06-ransac.pptx.pdf)<br><br>\n",
    "\n",
    "[**Orthophotos** by Prof. Dr. Cyrill Stachniss](https://www.ipb.uni-bonn.de/html/teaching/photo12-2021/2021-pho2-11-orthophoto.pptx.pdf)<br><br>\n",
    "\n",
    "[**Senior Lecturer Dr NAYEEM**](https://www.cs.umd.edu/~nayeem/)<br><br>\n",
    "\n",
    "[**Homography** by Mohammad Nayeem Teli](https://www.cs.umd.edu/class/fall2019/cmsc426-0201/files/16_Homography-estimation-and-decomposition.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homography estimation using DLT\n",
    "\n",
    "We will now estimate a transformation between each two images based on the found matches in the previous step. Since we are observing a planar object this transformation can be represented by an homography. <br><br>\n",
    "\n",
    "We need at least ***4*** corresponding 2D pints for fitting a 2D Homography transformation model.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "We want to implement the normalized DLT for 2D Homography estimation. <br><br>\n",
    "\n",
    "The normalized Direct Linear Transformation (DLT) algorithm is an extension of the standard DLT algorithm that accounts for differences in scaling and orientation between the two images. Here are the steps for the normalized DLT algorithm for 2D homography:<br>\n",
    "\n",
    "1. Normalize the coordinates: For each set of corresponding points, compute the centroid of the points in each set and scale them so that the average distance to the centroid is sqrt(2). This step helps to reduce numerical errors that can arise from large coordinate values.<br>\n",
    "2. Construct the matrix A: For each corresponding pair of normalized points *(x_i', y_i')* and *(x_i, y_i)*, construct a row in the matrix A as follows:<br>\n",
    "[ 0 0 0 -x_i' -y_i' -1 y_ix_i' y_iy_i' y_i ] <br>\n",
    "A_i = [ x_i' y_i' 1 0 0 0 -x_ix_i' -x_iy_i' -x_i ]<br>\n",
    "\n",
    "3. Compute the homography matrix H: Compute the null space of A using Singular Value Decomposition (SVD) and obtain the vector h that corresponds to the column of V that corresponds to the smallest singular value. Reshape h into a 3x3 matrix and normalize it so that the bottom right element is 1.<br>\n",
    "\n",
    "4. Denormalize the homography matrix: Undo the normalization that was applied in step 1 by computing the transformation that maps the normalized points to their original positions. This transformation can be computed using the centroids and scaling factors that were computed in step 1. <br>\n",
    "\n",
    "Note: The above steps assume that there are at least four corresponding points, which is the minimum number of points needed to estimate a homography matrix. If there are more than four points, the algorithm can be extended to use more points to improve the accuracy of the estimated homography.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the **normalized DLT algorithm for 2D homography using SIFT correspondences**, the steps are: <br>\n",
    "\n",
    "1. Extract the SIFT keypoints and descriptors for both images, which we have done in the previous step.<br>\n",
    "\n",
    "2. Use a matching algorithm, such as **brute force** matching, to find the correspondences between the SIFT descriptors in the two images. This will give you a set of matching keypoints in both images which we have also obtained in the previous step.<br>\n",
    "\n",
    "3. Choose at least 4 matching keypoints to use in the homography estimation. You can randomly select four keypoints, or choose them based on some criteria such as the number of inliers or the distribution of matches across the image.<br>\n",
    "\n",
    "4. Normalize the coordinates of the chosen keypoints by subtracting the centroid and dividing by the average distance to the centroid.<br>\n",
    "\n",
    "5. Construct the matrix A using the normalized coordinates of the chosen keypoints.<br>\n",
    "\n",
    "6. Use SVD to solve for the null space of A, which will give you the homography matrix H.<br>\n",
    "\n",
    "7. Denormalize the homography matrix H by reversing the normalization that was done in step 4.<br>\n",
    "\n",
    "8. Use the homography matrix H to warp one image onto the other using image warping techniques such as perspective transformation or affine transformation.<br><br>\n",
    "\n",
    "**Note:**  If the number of inliers is low or the matches are noisy, the homography estimation may be inaccurate. In this case, we can use RANSAC to improve the accuracy of the estimated homography."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cpu(pts):\n",
    "\n",
    "    \"\"\"\n",
    "    Normalize points so that the centroid is at the origin\n",
    "    and the average distance from the origin is sqrt(2).\n",
    "    \"\"\"\n",
    "    mean = np.mean(pts, axis=0)\n",
    "    dist = np.sqrt(np.sum((pts - mean)**2, axis=1)).mean()\n",
    "    T = np.array([[np.sqrt(2)/dist, 0, -mean[0]*np.sqrt(2)/dist],\n",
    "                [0, np.sqrt(2)/dist, -mean[1]*np.sqrt(2)/dist],\n",
    "                [0, 0, 1]])\n",
    "    return np.hstack((pts, np.ones((pts.shape[0], 1)))) @ T.T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_H_cpu(H, T1, T2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Denormalize the homography matrix H given the normalization\n",
    "    transformations T1 and T2.\n",
    "\n",
    "    Parameters:\n",
    "        :H: Homography Matrix\n",
    "        :T1 and T2: Normalization transformations\n",
    "        \n",
    "    Returns: \n",
    "        Denomarlized H matrix (3x3)\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(T2) @ H @ T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dlt_cpu(keypoints1, keypoints2, matches, k=4):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimate the homography matrix using normalized DLT (CPU).\n",
    "\n",
    "    :param keypoints1: representing the source points.\n",
    "    :param keypoints2: a numpy array of shape (N, 2) representing the destination points.\n",
    "    \n",
    "    :return: Homography matrix(3x3)\n",
    "    \"\"\"\n",
    "\n",
    "    # Choose at least 4 matching keypoints\n",
    "    kp1 = [keypoints1[m.queryIdx].pt for m in matches]\n",
    "    kp2 = [keypoints2[m.trainIdx].pt for m in matches]\n",
    "    indices = np.random.choice(len(kp1), k, replace=False)\n",
    "    pts1 = np.array([kp1[i] for i in indices])\n",
    "    pts2 = np.array([kp2[i] for i in indices])\n",
    "\n",
    "\n",
    "    # Normalize the coordinates of the chosen keypoints\n",
    "    pts1_norm, T1 = normalize_cpu(pts1)\n",
    "    pts2_norm, T2 = normalize_cpu(pts2)\n",
    "\n",
    "    # Construct the matrix A\n",
    "    A = []\n",
    "    for i in range(pts1_norm.shape[0]):\n",
    "        x1, y1, _ = pts1_norm[i]\n",
    "        x2, y2, _ = pts2_norm[i]\n",
    "        A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "        A.append([x1, y1, 1, 0, 0, 0, -x2*x1, -x2*y1, -x2])\n",
    "    A = np.array(A)\n",
    "\n",
    "    # Solve for the null space of A using SVD\n",
    "    _, _, V = svd(A)\n",
    "    H_norm = V[-1, :].reshape(3, 3)\n",
    "\n",
    "    # Denormalize the homography matrix\n",
    "    H = denormalize_H_cpu(H_norm, T1, T2)\n",
    "    np.set_printoptions(suppress=True)\n",
    "\n",
    "    return np.round(H, decimals=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation uses random sampling to select the 4 keypoints used in homography estimation, which can lead to different results each time it runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_gpu(pts):\n",
    "    \"\"\"\n",
    "    Normalize points so that the centroid is at the origin\n",
    "    and the average distance from the origin is sqrt(2).\n",
    "    \"\"\"\n",
    "    pts = pts.float()\n",
    "    mean = pts.mean(dim=0)\n",
    "    dist = torch.sqrt(torch.sum((pts - mean)**2, dim=1)).mean()\n",
    "    T = torch.tensor([[np.sqrt(2)/dist, 0, -mean[0]*np.sqrt(2)/dist],\n",
    "                    [0, np.sqrt(2)/dist, -mean[1]*np.sqrt(2)/dist],\n",
    "                    [0, 0, 1]])\n",
    "    T = T.to(pts.device)\n",
    "    return torch.cat((pts, torch.ones((pts.shape[0], 1), device=pts.device)), dim=1) @ T.T, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_H_gpu(H, T1, T2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Denormalize the homography matrix H given the normalization\n",
    "    transformations T1 and T2.\n",
    "\n",
    "    Parameters:\n",
    "        :H: Homography Matrix\n",
    "        :T1 and T2: Normalization transformations\n",
    "        \n",
    "    Returns: \n",
    "        Denomarlized H matrix (3x3)\n",
    "    \"\"\"\n",
    "    return torch.inverse(T2) @ H @ T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dlt_gpu(keypoints1, keypoints2, matches, k=4):\n",
    "\n",
    "    \"\"\"\n",
    "    Estimate the homography matrix using normalized DLT (GPU).\n",
    "\n",
    "    Parameters: \n",
    "\n",
    "        :param keypoints1: representing the source points.\n",
    "        :param keypoints2: a numpy array of shape (N, 2) representing the destination points.\n",
    "    \n",
    "    Returns: \n",
    "        Homography matrix(3x3) data type = tensor\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Choose 4 matching keypoints\n",
    "    matches_subset = np.random.choice(matches, k, replace=False)\n",
    "    kp1 = [keypoints1[m.queryIdx].pt for m in matches_subset]\n",
    "    kp2 = [keypoints2[m.trainIdx].pt for m in matches_subset]\n",
    "    pts1 = torch.tensor(kp1).to(device)\n",
    "    pts2 = torch.tensor(kp2).to(device)\n",
    "\n",
    "\n",
    "    # Normalize the coordinates of the chosen keypoints\n",
    "    pts1_norm, T1 = normalize_gpu(pts1)\n",
    "    pts2_norm, T2 = normalize_gpu(pts2)\n",
    "\n",
    "    # Construct the matrix A\n",
    "    A = torch.zeros((pts1_norm.shape[0]*2, 9), device=device)\n",
    "    for i in range(pts1_norm.shape[0]):\n",
    "        x1, y1, _ = pts1_norm[i]\n",
    "        x2, y2, _ = pts2_norm[i]\n",
    "        A[i*2] = torch.tensor([0, 0, 0, -x1, -y1, -1,\n",
    "                              y2*x1, y2*y1, y2], device=device)\n",
    "        A[i*2+1] = torch.tensor([x1, y1, 1, 0, 0,\n",
    "                                0, -x2*x1, -x2*y1, -x2], device=device)\n",
    "\n",
    "        # Solve for the null space of A using SVD\n",
    "    _, _, V = torch.svd(A)\n",
    "    H_norm = V[:, -1].reshape(3, 3)\n",
    "\n",
    "    # Denormalize the homography matrix\n",
    "    H = denormalize_H_gpu(H_norm, T1, T2)\n",
    "\n",
    "    return H\n",
    "\n",
    "H_GPU = normalized_dlt_gpu(keypoints_SIFT, keypoints_SIFT_1, img_1_good_matches_1_filtered)\n",
    "print(H_GPU)\n",
    "print(np.shape(H_GPU))\n",
    "\n",
    "H_GPU = to_numpy_array(H_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homography estimation using RANSAC\n",
    "\n",
    "We will now estimate a transformation between each two images based on the found matches in the previous step. Since we are observing a planar object this transformation can be represented by an homography. <br><br>\n",
    "\n",
    "We need at least ***4*** corresponding 2D pints for fitting a 2D Homography transformation model.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "We want to implement RANSAC for 2D Homography estimation. <br><br>\n",
    "\n",
    "Fischler and Bolles introduced the RANSAC algorithm in their 1981 paper, \"Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography.\" RANSAC is a general algorithm for fitting models to observed data that may contain outliers. The algorithm works by repeatedly selecting a random subset of the data and fitting a model to that subset, and then using that model to determine which of the remaining data points are inliers (consistent with the model) and which are outliers (inconsistent with the model). The process is repeated a fixed number of times, and the model with the most inliers is chosen as the best model.<br>\n",
    "\n",
    "The steps for RANSAC algorithm are:<br>\n",
    "\n",
    "1. Set the number of iterations, and threshold.<br>\n",
    "\n",
    "2. For i = 1 to N, Choose a random subset of the observed data.<br>\n",
    "\n",
    "3. Fit a model to the chosen subset.\n",
    "\n",
    "4. Determine the inliers: for each data point not in the chosen subset, determine if the point is consistent with the model. If the point is consistent (i.e., the error is below a threshold), add it to the inliers set. <br>\n",
    "\n",
    "5. If the number of inliers is above a threshold, re-fit the model to all the inliers and return it as the best model.<br>\n",
    "\n",
    "6. If the number of inliers is greater than some fraction p of the total number of data points, exit the loop and return the best estimate.<br>\n",
    "\n",
    "7. Choose the best model found while iterating N times.<br><br>\n",
    "\n",
    "We want to modify this algorithm to estimate the Homography matrix.<br>\n",
    "\n",
    "Here are the steps to implement RANSAC algorithm for estimating the homography matrix::<br>\n",
    "\n",
    "1. Randomly select a minimal set of points (usually 4) from the set of observed data points.<br>\n",
    "\n",
    "2. Compute the homography matrix using the selected points.<br>\n",
    "\n",
    "3. For each remaining data point, calculate the distance between the point and its projection onto the estimated homography.<br>\n",
    "\n",
    "4. Count the number of inliers (data points whose distances are less than a threshold value) and update the best homography if this number is larger than the previous iterations.<br>\n",
    "\n",
    "5. Repeat steps 1-4 for a fixed number of iterations or until the desired number of inliers is found. <br>\n",
    "\n",
    "6. Re-estimate the homography using all the inliers found in step 4. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dlt_cpu_ransac(keypoints1, keypoints2, matches, k=4, ransac_iters=100000, ransac_threshold=5.0):\n",
    "    best_H = None\n",
    "    best_inliers = None\n",
    "    num_inliers = 0\n",
    "    previous_num_inliers = -1\n",
    "\n",
    "    while num_inliers != previous_num_inliers:\n",
    "        previous_num_inliers = num_inliers\n",
    "\n",
    "        for i in range(ransac_iters):\n",
    "            # Estimate homography matrix using normalized DLT algorithm\n",
    "            H = normalized_dlt_cpu(keypoints1, keypoints2, matches, k)\n",
    "\n",
    "            # Calculate the number of inliers and outliers\n",
    "            inliers = []\n",
    "            outliers = []\n",
    "            for m in matches:\n",
    "                x1, y1 = keypoints1[m.queryIdx].pt\n",
    "                x2, y2 = keypoints2[m.trainIdx].pt\n",
    "                pt1_norm_h = np.array([x1, y1, 1.0])\n",
    "                pt2_norm_h_est = np.dot(H, pt1_norm_h)\n",
    "                pt2_norm_est = pt2_norm_h_est[:2] / pt2_norm_h_est[2]\n",
    "                pt2_norm_h = np.array([x2, y2])\n",
    "                dist = np.linalg.norm(pt2_norm_h - pt2_norm_est)\n",
    "                if dist <= ransac_threshold:\n",
    "                    inliers.append(m)\n",
    "                else:\n",
    "                    outliers.append(m)\n",
    "\n",
    "            # Update the best model if the current one has more inliers\n",
    "            if len(inliers) > num_inliers:\n",
    "                best_H = H\n",
    "                best_inliers = inliers\n",
    "                num_inliers = len(inliers)\n",
    "\n",
    "    return best_H, best_inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_dlt_ransac_gpu(keypoints1, keypoints2, matches, num_iterations=1000, threshold=5):\n",
    "    \"\"\"\n",
    "    Estimate the homography matrix using RANSAC algorithm.\n",
    "\n",
    "    :param keypoints1: representing the source points.\n",
    "    :param keypoints2: a numpy array of shape (N, 2) representing the destination points.\n",
    "    :param num_iterations: the maximum number of iterations for RANSAC algorithm.\n",
    "    :param threshold: the maximum distance from a point to the estimated homography to be considered an inlier.\n",
    "    :param sample_size: the size of the random subset of points to be used for estimating the homography.\n",
    "    :return: a tuple of (best_model, best_inliers), where best_model is the estimated homography and best_inliers\n",
    "    is a numpy array of shape (M, 2) representing the inliers.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_H = None\n",
    "    best_num_inliers = 0\n",
    "    previous_num_inliers = -1\n",
    "\n",
    "    while best_num_inliers != previous_num_inliers:\n",
    "        previous_num_inliers = best_num_inliers\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "\n",
    "            H = normalized_dlt_gpu(keypoints1, keypoints2, matches)\n",
    "\n",
    "            # Compute the number of inliers\n",
    "            num_inliers = 0\n",
    "            for match in matches:\n",
    "                x1, y1 = keypoints1[match.queryIdx].pt\n",
    "                x2, y2 = keypoints2[match.trainIdx].pt\n",
    "                pt1 = torch.tensor([x1, y1, 1], device=device)\n",
    "                pt2 = torch.tensor([x2, y2, 1], device=device)\n",
    "                pt2_est = H @ pt1\n",
    "                pt2_est_copy = pt2_est.clone()\n",
    "                pt2_est_copy /= pt2_est[2]\n",
    "                error = torch.sqrt(torch.sum((pt2 - pt2_est_copy)**2)).item()\n",
    "                if error < threshold:\n",
    "                    num_inliers += 1\n",
    "\n",
    "            # Update the best homography matrix if the current one is better\n",
    "                if num_inliers > best_num_inliers:\n",
    "                    best_H = H\n",
    "                    best_num_inliers = num_inliers\n",
    "\n",
    "            \n",
    "            \n",
    "    # inliner_ratio = float(np.sum(num_inliers)) / len(matches)\n",
    "    # print(\"The Inliner Ratio is: \",inliner_ratio)\n",
    "    # num_iterations_95 = np.log(1 - 0.95) / np.log(1 - inliner_ratio ** 4)\n",
    "    # print(\"The number of iterations required for 95 percent confidence is: \", num_iterations_95)\n",
    "    # num_iterations_99 = np.log(1 - 0.99) / np.log(1 - inliner_ratio ** 4)\n",
    "    # print(\"The number of iterations required for 99 percent confidence is: \",num_iterations_99)\n",
    "    \n",
    "    torch.set_printoptions(precision=3, sci_mode=False)\n",
    "\n",
    "    return best_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_homography_using_Built_in(keypoints1, keypoints2, matches, num_iterations=1000, threshold=5.0, num_inliers=4):\n",
    "    \"\"\"\n",
    "    Computes the homography matrix using RANSAC algorithm from a set of keypoint matches.\n",
    "\n",
    "    Args:\n",
    "        keypoints1: list of cv2.KeyPoint objects in image1\n",
    "        keypoints2: list of cv2.KeyPoint objects in image2\n",
    "        matches: list of cv2.DMatch objects\n",
    "        num_iterations: maximum number of iterations to perform\n",
    "        threshold: maximum distance between points for them to be considered inliers\n",
    "        num_inliers: minimum number of inliers required to re-estimate the homography\n",
    "\n",
    "    Returns:\n",
    "        The homography matrix and the indices of inlier matches.\n",
    "    \"\"\"\n",
    "\n",
    "    num_matches = len(matches)\n",
    "    best_H = None\n",
    "    best_inliers = []\n",
    "    for i in range(num_iterations):\n",
    "        # Randomly select 4 matches\n",
    "        sample = random.sample(matches, 4)\n",
    "\n",
    "        # Extract corresponding points\n",
    "        pts1 = np.float32([keypoints1[m.queryIdx].pt for m in sample])\n",
    "        pts2 = np.float32([keypoints2[m.trainIdx].pt for m in sample])\n",
    "\n",
    "        # Compute homography matrix\n",
    "        H, _ = cv2.findHomography(pts1, pts2, cv2.RANSAC, threshold)\n",
    "\n",
    "        # Find inliers\n",
    "        sample_inliers = []\n",
    "        for j, match in enumerate(matches):\n",
    "            pt1 = np.float32(keypoints1[match.queryIdx].pt).reshape(-1, 1)\n",
    "            pt2 = np.float32(keypoints2[match.trainIdx].pt).reshape(-1, 1)\n",
    "\n",
    "            # Append a 1 to the end of pt2 to make it a 3x1 vector\n",
    "            pt2_hom = np.vstack((pt2, 1))\n",
    "\n",
    "            # Compute distance between transformed point and its match in the other image\n",
    "            pt1_hom = np.vstack((pt1, 1))\n",
    "            d = np.linalg.norm(H @ pt1_hom - pt2_hom)\n",
    "\n",
    "            # If the distance is below the threshold, the match is an inlier\n",
    "            if d < threshold:\n",
    "                sample_inliers.append(j)\n",
    "        w = num_inliers / num_matches\n",
    "# Compute the number of iterations required for 95% confidence\n",
    "        p = 0.95\n",
    "        k = 4\n",
    "        num_iterations = int(np.log(1 - p) / np.log(1 - w ** k))\n",
    "        # If enough inliers were found, re-estimate the homography using all inliers\n",
    "        if len(sample_inliers) >= num_inliers:\n",
    "            pts1 = np.float32([keypoints1[matches[j].queryIdx].pt for j in sample_inliers])\n",
    "            pts2 = np.float32([keypoints2[matches[j].trainIdx].pt for j in sample_inliers])\n",
    "            H, _ = cv2.findHomography(pts1, pts2, cv2.RANSAC, threshold)\n",
    "\n",
    "            # If the new homography has more inliers than the previous one, update the best homography and inliers\n",
    "            if len(sample_inliers) > len(best_inliers):\n",
    "                best_H = H\n",
    "                best_inliers = sample_inliers\n",
    "\n",
    "    return best_H, best_inliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Question:** Discuss and indicate the parameters you need to select in order to compute one iteration of RANSAC to fit your descriptors matches. How many iterations would you select for finding the model with either 95% and 99% confidence?\n",
    "\n",
    "**Answer:** To perform one iteration of RANSAC to fit a homography model to a set of descriptors matches, we need to select the following parameters:<br>\n",
    "\n",
    "- ***n***: the minimum number of matches required to estimate the model. For a homography model, n should be 4 since a homography can be estimated from four correspondences.<br>\n",
    "\n",
    "- ***k***: the number of random samples to be selected in each iteration. k should be chosen such that there is a high probability that at least one of the samples is free of outliers. A common value for k is 4, which corresponds to the minimum number of correspondences required to compute a homography.<br>\n",
    "\n",
    "- ***Threshold***: the maximum distance between a correspondence and its projection onto the estimated model that is considered an inlier. The value of threshold should be chosen based on the expected noise in the data and the desired confidence level.\n",
    "\n",
    "The number of iterations required to find a model with either 95% or 99% confidence depends on the percentage of inliers in the data, the number of matches, and the threshold value.<br>\n",
    "\n",
    "The number of iterations required can be estimated using the following equation:<br>\n",
    "\n",
    "\n",
    "$$ num_{iterations} = \\frac{log(1 - confidence)}{log(1 - (inlier_{ratio})^n)} $$ \n",
    "\n",
    "where:\n",
    "- ***Confidence*** is the desired confidence level, <br>\n",
    "\n",
    "- ***$inlier_{ratio}$***: ratio of inliers to total matches, <br>\n",
    "\n",
    "- ***n*** is the minimum number of matches required to estimate the model,<br>\n",
    "\n",
    "- log is the natural logarithm.<br>\n",
    "\n",
    "For example, if the inlier ratio is 0.5, the minimum number of matches is 4, and the desired confidence level is 95%, we can compute the number of iterations as: <br>\n",
    "$$ num_iterations = \\frac{log(1 - 0.95)}{log(1 - 0.5 ** 4)} = 11 $$\n",
    "\n",
    "Similarly, if we want a 99% confidence level, we can compute the number of iterations as:<br>\n",
    "$$ num_iterations = \\frac{log(1 - 0.99)}{log(1 - 0.5 ** 4)} = 17 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping one image to another Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_warped_texture(H, img_ref, img_tgt, patch_texture):\n",
    "    # Warp patch texture that will be placed in the selected region with the estimated homography transformation H\n",
    "    h, w, _ = img_ref.shape\n",
    "    patch_texture_res = cv2.resize(patch_texture, (w, h))\n",
    "    warped_patch = cv2.warpPerspective(patch_texture_res, H, (img_tgt.shape[1], img_tgt.shape[0]))\n",
    "\n",
    "    # Remove the pixels of the foreground that will be replaced (we keep only background)\n",
    "    mask = (255 * np.ones((h, w))).astype(np.uint8)\n",
    "    mask_warped = cv2.warpPerspective(mask, H, (img_tgt.shape[1], img_tgt.shape[0]), flags=cv2.INTER_NEAREST)\n",
    "    mask_background = (1 - mask_warped / 255).astype(np.uint8)\n",
    "    mask_background = cv2.merge((mask_background, mask_background, mask_background))\n",
    "    background = cv2.multiply(mask_background, img_tgt)\n",
    "\n",
    "    # Blend images to a single frame and display\n",
    "    blend_img = cv2.add(background, warped_patch)\n",
    "\n",
    "    return blend_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wrapping(ref_image, original_image, new_image):\n",
    "    \n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    matcher = cv2.BFMatcher()\n",
    "\n",
    "    reference_keypoints, reference_descriptors = sift.detectAndCompute(ref_image, None)\n",
    "\n",
    "    original_keypoints, original_descriptors = sift.detectAndCompute(original_image, None)\n",
    "    matches = matcher.knnMatch(reference_descriptors, original_descriptors, k=2)\n",
    "\n",
    "    # Apply ratio test to filter out poor matches\n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    reference_points = np.float32([reference_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    original_points = np.float32([original_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    H, mask = cv2.findHomography(reference_points, original_points, cv2.RANSAC, 5.0)\n",
    "    result = render_warped_texture(H, ref_image, original_image, new_image)\n",
    "    return  result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keypoints_subplot(image, result1, result2, result3, result4, x=20):\n",
    "    \"\"\"\n",
    "    Plots the results.\n",
    "\n",
    "    Args\n",
    "        result1: \n",
    "        result2: \n",
    "        result3: \n",
    "        result4: \n",
    "        x: optional parameter for font size in the plot titles (default: 20) \n",
    "\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(\n",
    "        image.shape[1]/30, image.shape[0]/30))\n",
    "    ax[0, 0].imshow(cv2.cvtColor(result1, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[0, 0].axis('off')\n",
    "    ax[0, 1].imshow(cv2.cvtColor(result2, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[0, 1].axis('off')\n",
    "    ax[1, 0].imshow(cv2.cvtColor(result3, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[1, 0].axis('off')\n",
    "    ax[1, 1].imshow(cv2.cvtColor(result4, cv2.COLOR_BGR2RGB), aspect='equal')\n",
    "    ax[1, 1].axis('off')\n",
    "    ax[0, 0].set_title('Image 1', fontsize=x)\n",
    "    ax[0, 1].set_title('Image 2', fontsize=x)\n",
    "    ax[1, 0].set_title('Image 3', fontsize=x)\n",
    "    ax[1, 1].set_title('Image 4', fontsize=x)\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_CPU = normalized_dlt_cpu(keypoints_SIFT, keypoints_SIFT_1, img_1_good_matches_1_filtered)\n",
    "print(H_CPU)\n",
    "print(np.shape(H_CPU))\n",
    "\n",
    "pic = render_warped_texture(H_CPU, ref_image, img_1, E_logo)\n",
    "plt.figure(figsize =(20 ,10))\n",
    "plt.imshow( cv2.cvtColor(pic , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC algorithm for estimating the homography matrix using DLT Homography estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_best = normalized_dlt_ransac_gpu(keypoints_SIFT, keypoints_SIFT_1, img_1_good_matches_1_filtered)\n",
    "\n",
    "# Convert to numpy array\n",
    "H_best = to_numpy_array(H_best)\n",
    "print(H_best)\n",
    "print(np.shape(H_best))\n",
    "pic = render_warped_texture(H_best, ref_image, img_1, E_logo)\n",
    "\n",
    "plt.figure(figsize =(20 ,10))\n",
    "plt.imshow( cv2.cvtColor(pic , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_HH, best_inliers = normalized_dlt_cpu_ransac(keypoints_SIFT, keypoints_SIFT_4, img_4_good_matches_1, k=4, ransac_iters=1000, ransac_threshold=5.0)\n",
    "print(best_HH)\n",
    "\n",
    "\n",
    "\n",
    "pic = render_warped_texture(best_HH, ref_image, img_4, E_logo)\n",
    "plt.figure(figsize =(20 ,10))\n",
    "plt.imshow( cv2.cvtColor(pic , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping With Homography matrix using Built in function from OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_H, best_inliers = ransac_homography_using_Built_in(keypoints_SIFT, keypoints_SIFT_3, img_3_good_matches_1_filtered, num_iterations=1000, threshold=5.0, num_inliers=4)\n",
    "print(best_H)\n",
    "pic = render_warped_texture(best_H, ref_image, img_3, E_logo)\n",
    "\n",
    "plt.figure(figsize =(20 ,10))\n",
    "plt.imshow( cv2.cvtColor(pic , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying wrapping to all images\n",
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_logo_2 = cv2.imread('data/Ferrari front.jpg')\n",
    "E_logo_3 = cv2.imread('data/Mercedes-w14.jpg')\n",
    "\n",
    "result_1 = apply_wrapping(ref_image, img_1, E_logo)\n",
    "result_2 = apply_wrapping(ref_image, img_2, E_logo)\n",
    "result_3 = apply_wrapping(ref_image, img_3, E_logo)\n",
    "result_4 = apply_wrapping(ref_image, img_4, E_logo)\n",
    "plot_keypoints_subplot(img_1, result_1, result_2, result_3, result_4, x=50)\n",
    "\n",
    "result_1 = apply_wrapping(ref_image, img_1, E_logo_2)\n",
    "result_2 = apply_wrapping(ref_image, img_2, E_logo_2)\n",
    "result_3 = apply_wrapping(ref_image, img_3, E_logo_2)\n",
    "result_4 = apply_wrapping(ref_image, img_4, E_logo_2)\n",
    "plot_keypoints_subplot(img_1, result_1, result_2, result_3, result_4, x=50)\n",
    "\n",
    "result_1 = apply_wrapping(ref_image, img_1, E_logo_3)\n",
    "result_2 = apply_wrapping(ref_image, img_2, E_logo_3)\n",
    "result_3 = apply_wrapping(ref_image, img_3, E_logo_3)\n",
    "result_4 = apply_wrapping(ref_image, img_4, E_logo_3)\n",
    "plot_keypoints_subplot(img_1, result_1, result_2, result_3, result_4, x=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus TASK\n",
    "## With FAST and ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FAST_ORB(ref_image, original_image, new_image):\n",
    "    \n",
    "    fast = cv2.FastFeatureDetector_create()\n",
    "    orb = cv2.ORB_create()\n",
    "    matcher = cv2.BFMatcher()\n",
    "\n",
    "    reference_keypoints = fast.detect(ref_image, None)\n",
    "    reference_keypoints, reference_descriptors = orb.compute(ref_image, reference_keypoints)\n",
    "\n",
    "    original_keypoints = fast.detect(original_image, None)\n",
    "    original_keypoints, original_descriptors = orb.compute(original_image, original_keypoints)\n",
    "\n",
    "    matches = matcher.knnMatch(reference_descriptors, original_descriptors, k=2)\n",
    "\n",
    "    # Apply ratio test to filter out poor matches\n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    reference_points = np.float32([reference_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    original_points = np.float32([original_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    H, mask = cv2.findHomography(reference_points, original_points, cv2.RANSAC, 5.0)\n",
    "    result = render_warped_texture(H, ref_image, original_image, new_image)\n",
    "    return  result, reference_keypoints, reference_descriptors, original_keypoints, original_descriptors, good_matches, H\n",
    "\n",
    "FAST_ORB, reference_keypoints, \\\n",
    "reference_descriptors, original_keypoints,\\\n",
    "original_descriptors, good_matches, H_FAST_ORB = \\\n",
    "        FAST_ORB(ref_image, img_2, E_logo)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( cv2.cvtColor(FAST_ORB , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.title('Esirum Logo Wrapped Using FAST and ORB')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow( cv2.cvtColor(drawKeypoints(ref_image, reference_keypoints) , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.title('Key points of FAST on reference image')\n",
    "plt.show()\n",
    "\n",
    "matched_image = cv2.drawMatches(ref_image, reference_keypoints, img_2, original_keypoints, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure()\n",
    "plt.imshow( cv2.cvtColor(matched_image , cv2.COLOR_BGR2RGB ) )\n",
    "plt.axis('off')\n",
    "plt.title('Matches reference image and Image 2 using ORB detector')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1f70c03a818578fafa160523bb9c397cdfd0531f87bbd4ce6ceae6031b301a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
